{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Market Basket Analysis: Customer's Diet and Their Next Order List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before doing any extensive analysis, I made a list of questions that got me curious about the data. I used this list as a guide in my analysis.  \n",
    "\n",
    "### QUESTIONS TO ANSWER USING THE DATA\n",
    "1.\tHow many products?\n",
    "2.\tHow many aisles?\n",
    "3.\tHow many department?\n",
    "4.\tHow many customers?\n",
    "5.\tHow many total orders?\n",
    "6.\tAre there missing data? What type of missing data?\n",
    "7.\tWhen are the peak hours (orders>100,000)? When is the orders highest and lowest?\n",
    "8.\tWhat day of the week has the highest and lowest order volume?\n",
    "9.\tWhat is the probability of each product being ordered?\n",
    "10.\tWhat is the probability of each department being ordered from?\n",
    "11.\tWhat is the probability of each aisle being ordered from?\n",
    "12.\tCan I identify meat eaters, vegetarian, vegan their percentage in the entire customer list? (Hypothesis Testing is in section)\n",
    "13.\tWhat is the probability of customers being meat eater, vegetarian or vegan? (I used Bayesian Statistic in this part)\n",
    "14.\tWhat products appear in all customer A orders? – These products will have high probability being reordered by customer A\n",
    "15.\tHow many orders for each customer?\n",
    "16.\tWhat is the average number of products for across all orders for each customer?\n",
    "17. Using average number of product per order for each customer and probability of product to be reordered by customer, can I predict products that will be reordered by customer?\n",
    "18.\tWhat is the accuracy of my predicted next order list to the actual next order list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "from scipy import stats\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'products.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4586657bbad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'products.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pamel\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pamel\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pamel\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pamel\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pamel\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'products.csv' does not exist"
     ]
    }
   ],
   "source": [
    "prod=pd.read_csv('products.csv')\n",
    "prod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. How many products? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod.product_id.max()  #number of products available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. How many aisles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod.aisle_id.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. How many departments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod.department_id.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orders=pd.read_csv('orders.csv')\n",
    "orders.head()    #order_dow days of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. How many customers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orders.user_id.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. How many total orders?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orders.order_id.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Maximum number of order for a customer\n",
    "orders.order_number.max()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## User_id of customer with maximum order of 100\n",
    "orders.set_index('user_id').order_number.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-Q5 ANSWERS\n",
    "There are a total of 7 csv files used in data analysis namely: \n",
    "1) aisles, 2) departments, 3) order_products_prior, 4) order_products_train, 5) orders, 6) products, 7) sample_submission\n",
    "\n",
    "Querying 5) orders and 6) products we now know that there are \n",
    "\n",
    "### 49,688      PRODUCTS\n",
    "### 134           AISLES\n",
    "### 21             DEPARMENTS\n",
    "### 206,209    CUSTOMERS\n",
    "### 3,421,083 ORDERS\n",
    "\n",
    "Customer with user_id 210 has odered the most of 100 orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orders dataframe has 3 eval_set (prior, train, test)\n",
    "prior has the most number of orders and contains order history of users while train and set has latest orders of selected users that can be use for training a model and testing a model\n",
    "\n",
    "Prior Set= 3,214,874 orders,  Train Set = 131, 209 orders, Test Set =75,000 orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orders.eval_set.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orders dataframe is separated to the 3 eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oprior=orders[orders.eval_set=='prior']\n",
    "otrain=orders[orders.eval_set=='train']\n",
    "otest=orders[orders.eval_set=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Are there missing data? \n",
    "### Yes only in orders dataframe. There are - 206, 209 \"NaN\" in days_since_prior_order  column which are the first order of all 206,209 users. This is a MAR (missing at random) type of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orders.isnull().sum()      #missing values in days_since_prior_order = users first order in INSTACART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aisles=pd.read_csv('aisles.csv')\n",
    "aisles.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep=pd.read_csv('departments.csv')     \n",
    "dep.isnull().sum()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior=pd.read_csv('order_products__prior.csv')\n",
    "prior.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('order_products__train.csv')\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp=pd.read_csv('sample_submission.csv')\n",
    "samp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. When are peak hours when order>100,000? (8am to 10pm) \n",
    "\n",
    "### When are orders highest and lowest? (lowest at 3am and highest at 10am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourdist=orders['order_hour_of_day'].value_counts().sort_index()\n",
    "hourdist.plot(kind='bar')\n",
    "_=plt.xlabel('hour of the day')\n",
    "_=plt.ylabel('order volume')\n",
    "_=plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])\n",
    "plt.title('Daily Order Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## These are the peak hours (8-20 orders are over 100,000)\n",
    "hourdist[hourdist>100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourdist.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourdist.idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. What day of the week is order volume higest and lowest?                                                    \n",
    "\n",
    "## (Highest on Mondays and lowest on Fridays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orday=orders.order_dow.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orday.sort_index().plot(kind='bar', color='r')\n",
    "_=plt.xlabel('day of week')\n",
    "_=plt.ylabel('order volume')\n",
    "_=plt.xticks([0,1,2,3,4,5,6], ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.title('Weekly Order Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orday.idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orday.idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7-Q8 ANSWERS\n",
    "Peak hours where orders are above 100,000 / hour are at 8am to 10 pm.  Highest order is at 10 am and lowest is at 3am. Order volume are at high on Mondays and Tuesdays, starts slowing down on Wednesdays, at minimum on Fridays and picks up a little on the weekend.  This means that the website must be at optimum performance on peak hours 8am-10pm and the days when order volume are at high.  When choosing what day and hour to do the website maintenance that will require down time, Friday between 12am to 6am will be the best day and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MERGING DATAFRAMES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING \"otrain\" dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## alltrain has latest order of selected users but does not include their order history\n",
    "alltrain=otrain.merge(train).merge(prod).merge(dep).merge(aisles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alltrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number rows,columns in alltrain\n",
    "alltrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of user_id in alltrain\n",
    "alltrain.user_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of orders in alltrain\n",
    "alltrain.order_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Average number of products per order in alltrain\n",
    "alltrain.groupby('order_id')['product_name'].size().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### USING \"oprior\" dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allprior=oprior.merge(prior).merge(prod).merge(dep).merge(aisles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allprior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of rows, columns in allprior\n",
    "allprior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of user_id in allprior\n",
    "allprior.user_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of orders in allprior\n",
    "allprior.order_id.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. What is the probability of a specific product being ordered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Probability of Products being ordered using \"alltrain\"\n",
    "ProbA=alltrain.product_name.value_counts()\n",
    "ProbA=pd.DataFrame(ProbA)\n",
    "ProbA['PA(ordered)']=ProbA['product_name']/alltrain.product_name.size\n",
    "ProbA=ProbA.reset_index()\n",
    "ProbA.columns=['product_name','count','PA(ordered)']\n",
    "ProbA.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Probability of Products being ordered using \"allprior\"\n",
    "ProbB=allprior.product_name.value_counts()\n",
    "ProbB=pd.DataFrame(ProbB)\n",
    "ProbB['PB(ordered)']=ProbB['product_name']/allprior.product_name.size\n",
    "ProbB=ProbB.reset_index()\n",
    "ProbB.columns=['product_name','count','PB(ordered)']\n",
    "ProbB.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. What is the probability of each department being ordered from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Probability of Department being ordered from using \"alltrain\"\n",
    "ProbDA=alltrain.department.value_counts()\n",
    "ProbDA=pd.DataFrame(ProbDA)\n",
    "ProbDA['PDA(ordered)']=ProbDA['department']/alltrain.department.size\n",
    "ProbDA.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Probability of Department being ordered from using \"allprior\"\n",
    "ProbDB=allprior.department.value_counts()\n",
    "ProbDB=pd.DataFrame(ProbDB)\n",
    "ProbDB['PDB(ordered)']=ProbDB['department']/allprior.department.size\n",
    "ProbDB.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. What is the probability of each aisle being ordered from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Probability of Aisle being ordered from using \"alltrain\"\n",
    "aisle_percenta=alltrain.aisle_id.value_counts()/alltrain.aisle_id.size\n",
    "aisle_percenta=pd.DataFrame(aisle_percenta).sort_index().reset_index()\n",
    "aisle_percenta.columns=['aisle_id', 'aisle_percent_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Percentage/probability of orders coming from each aisle using allprior \n",
    "aisle_percentb=allprior.aisle_id.value_counts()/allprior.aisle_id.size\n",
    "aisle_percentb=pd.DataFrame(aisle_percentb).sort_index().reset_index()\n",
    "aisle_percentb.columns=['aisle_id', 'aisle_percent_prior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aisle_percent=aisle_percenta.merge(aisle_percentb)\n",
    "aisle_percent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "y=np.arange(134)\n",
    "plt.barh(range(len(aisle_percenta.aisle_percent_train)), aisle_percenta.aisle_percent_train, color='b', alpha=0.5)\n",
    "plt.barh(range(len(aisle_percentb.aisle_percent_prior)), aisle_percentb.aisle_percent_prior, color='y', alpha=0.5)\n",
    "plt.ylim(0,135)\n",
    "plt.ylabel('Percent')\n",
    "plt.ylabel('aisle_id')\n",
    "plt.title('Aisle Distribution')\n",
    "plt.legend(['train', 'prior'], loc='upper right')\n",
    "ax.set_yticks(np.arange(0,136,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Can you classify the customer as MEAT_LOVERS, PESCATARIAN, NONVEGAN AND VEGAN Vegetarians?\n",
    "\n",
    "I have simplified classifying these 4 groups by looking at what aisles customers order from. \n",
    "Meat_Lovers will for sure have meat and seafood products from aisles 5, 7, 15, 34, 35, 39, 49, 95, 96, 106 and 122 \n",
    "Pescatarians will for sure only have seafood products from aisles 15, 34, 39 and 95\n",
    "NonVegans will not eat any from the meat and seafood from aisles 5, 7, 15, 34, 35, 39, 49, 95, 96, 106 and 122\n",
    "\n",
    "Vegans will not eat any from the meat and seafood from aisles 5, 7, 15, 34, 35, 39, 49, 95, 96, 106 and 122 and any dairy products like milk, cheeses and creamsin aisles 2, 21, 53, 84, 86, 108, and 120\n",
    "\n",
    "#### Q12A. Different Functions are written that will be used to classify the four diet groups\n",
    "#### Q12B. Diet Classifications will be identified by USERS\n",
    "#### Q12C. Diet Classifications will be identified by ORDERS\n",
    "#### Q12D. Generation of Simulated Datasets\n",
    "#### Q12E. Hypothesis Testing part 1: Are there significant differences between Diet distributions classified by USERS vs. by ORDERS?\n",
    "#### Q12F: hypothesis testing part 2: Are there significant differences between Diet distribution classified by USERS and by ORDERS using Empirical dataset vs. Simulated dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## U is the dataframe of users with their maximum number of orders using \"oprior\"\n",
    "U=oprior.groupby('user_id')['order_number'].agg(['max'])\n",
    "U['user_id']=U.index.get_level_values('user_id').values\n",
    "U.columns=['max_order', 'user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## P_num is the dataframe of users with order_id and the number of products per order using \"allprior\"\n",
    "P_num=allprior.groupby('user_id')['order_id'].value_counts()\n",
    "P_num=pd.DataFrame(P_num)\n",
    "P_num.columns=['prod_per_order']\n",
    "P_num['user_id']=P_num.index.get_level_values('user_id').values\n",
    "P_num['order_id']=P_num.index.get_level_values('order_id').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Tot_p is the dataframe of users with total number of products they ordered using allprior\n",
    "Tot_p=allprior.user_id.value_counts()\n",
    "Tot_p=pd.DataFrame(Tot_p).reset_index()\n",
    "Tot_p.columns=['user_id', 'total_products']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12A. Functions used to identify Diet Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1. This function provides a sample of \"n\" random users from the oprior dataframe for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sample_maker(Q):  ## User_Max_Or is s dataframe with user_id and their maximum order_number, n is number of samples\n",
    "    PO=Q.merge(P_num).merge(Tot_p)\n",
    "    sample1=Q.sample(n=30000, replace=False, random_state=0, axis=0)\n",
    "    \n",
    "    ## sample1 is the DataFrame of the first dataset\n",
    "    sample1=sample1.merge(PO, how='inner')\n",
    "    sample1=sample1.loc[:,['user_id', 'order_id', 'max_order', 'prod_per_order', 'total_products']]\n",
    "\n",
    "\n",
    "    #Emp is the Empirical dataframe\n",
    "    Emp=sample1.merge(allprior, how='inner')\n",
    "    \n",
    "    return Emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F2. This Function classifies Users as Meat_Lovers, Pescatarian, Vegan, NonVegan according to users overall order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Diet_Class_user(A): ## A is a  dataframe produced from \"Sample_maker\" function\n",
    "    ## Total products ordered from Meat & Seafood Aisles\n",
    "    MS=A[(A.aisle_id==5)|(A.aisle_id==7)|(A.aisle_id==15)|(A.aisle_id==34)|(A.aisle_id==35)|(A.aisle_id==39)|(A.aisle_id==49)|(A.aisle_id==95)|(A.aisle_id==96)|(A.aisle_id==106)|(A.aisle_id==122)]\n",
    "    \n",
    "    ##Eats Meat and Fish\n",
    "    M=MS[(MS.aisle_id==5)|(A.aisle_id==7)|(A.aisle_id==35)|(A.aisle_id==49)|(A.aisle_id==96)|(A.aisle_id==106)|(A.aisle_id==122)]\n",
    "    MF=M.user_id.unique()\n",
    "    Meat_L=pd.DataFrame(MF, columns=['user_id'])\n",
    "    Meat_L['Diet']='Meat_Lovers'\n",
    "    \n",
    "    ##Pescatarian that eat and not eat other meat\n",
    "    P=MS[(MS.aisle_id==15)|(MS.aisle_id==34)|(MS.aisle_id==39)|(MS.aisle_id==95)]\n",
    "    F=P.user_id.unique()\n",
    "    \n",
    "    ## Pescatarian customers\n",
    "    Pesca=np.setdiff1d(F,MF)\n",
    "    Pesca=pd.DataFrame(Pesca, columns=['user_id'])\n",
    "    Pesca['Diet']='Pescatarian'\n",
    "\n",
    "    ## All Vegetarians\n",
    "    Veg=A.loc[~A.user_id.isin(MS.user_id)]\n",
    "\n",
    "    # NonVegan \n",
    "    NV=Veg[(Veg.aisle_id==86)|(Veg.aisle_id==2)|(Veg.aisle_id==21)|(Veg.aisle_id==53)|(Veg.aisle_id==84)|(Veg.aisle_id==108)|(Veg.aisle_id==120)]\n",
    "    F1=NV.user_id.unique()\n",
    "\n",
    "    NonVeg=pd.DataFrame(F1, columns=['user_id'])\n",
    "    NonVeg['Diet']='NonVegan'\n",
    "    \n",
    "    ##Vegans\n",
    "    Vegans=Veg.loc[~Veg.user_id.isin(F1)]\n",
    "    Vega=Vegans.user_id.unique()\n",
    "    Vega=pd.DataFrame(Vega, columns=['user_id'])\n",
    "    Vega['Diet']='Vegan'\n",
    "    \n",
    "    ## Merge all DataFrame of Different Diets\n",
    "    Sample_class=pd.concat([Meat_L, Pesca, NonVeg, Vega])\n",
    "    \n",
    "    return Sample_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F3. This Function classifies Meat_Lovers, Pescatarian, Vegan, NonVegan according to orders and disregarding who ordered it (the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Diet_Class_orders(A):\n",
    "    ## Total products ordered from Meat & Seafood Aisles\n",
    "    MS=A[(A.aisle_id==5)|(A.aisle_id==7)|(A.aisle_id==15)|(A.aisle_id==34)|(A.aisle_id==35)|(A.aisle_id==39)|(A.aisle_id==49)|(A.aisle_id==95)|(A.aisle_id==96)|(A.aisle_id==106)|(A.aisle_id==122)]\n",
    "    \n",
    "    ##Eats Meat and Fish\n",
    "    M=MS[(MS.aisle_id==5)|(A.aisle_id==7)|(A.aisle_id==35)|(A.aisle_id==49)|(A.aisle_id==96)|(A.aisle_id==106)|(A.aisle_id==122)]\n",
    "    MF=M.order_id.unique()\n",
    "    Meat_L=pd.DataFrame(MF, columns=['order_id'])\n",
    "    Meat_L['Diet']='Meat_Lovers'\n",
    "    \n",
    "    ##Pescatarian that eat and not eat other meat\n",
    "    P=MS[(MS.aisle_id==15)|(MS.aisle_id==34)|(MS.aisle_id==39)|(MS.aisle_id==95)]\n",
    "    F=P.order_id.unique()\n",
    "    \n",
    "    ## Pescatarian customers\n",
    "    Pesca=np.setdiff1d(F,MF)\n",
    "    Pesca=pd.DataFrame(Pesca, columns=['order_id'])\n",
    "    Pesca['Diet']='Pescatarian'\n",
    "\n",
    "    ## All Vegetarians\n",
    "    Veg=A.loc[~A.order_id.isin(MS.order_id)]\n",
    "\n",
    "    # NonVegan \n",
    "    NV=Veg[(Veg.aisle_id==86)|(Veg.aisle_id==2)|(Veg.aisle_id==21)|(Veg.aisle_id==53)|(Veg.aisle_id==84)|(Veg.aisle_id==108)|(Veg.aisle_id==120)]\n",
    "    F1=NV.order_id.unique()\n",
    "\n",
    "    NonVeg=pd.DataFrame(F1, columns=['order_id'])\n",
    "    NonVeg['Diet']='NonVegan'\n",
    "    \n",
    "    ##Vegans\n",
    "    Vegans=Veg.loc[~Veg.order_id.isin(F1)]\n",
    "    Vega=Vegans.order_id.unique()\n",
    "    Vega=pd.DataFrame(Vega, columns=['order_id'])\n",
    "    Vega['Diet']='Vegan'\n",
    "    \n",
    "    ## Merge all DataFrame of Different Diets\n",
    "    Sample_class=pd.concat([Meat_L, Pesca, NonVeg, Vega])\n",
    "    \n",
    "    return Sample_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F4. This function returns a horizontal bar graph of distrubution of the four Diet Categories of Customers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Diet_Percentage(B):\n",
    "    Per=B.Diet.value_counts()\n",
    "    Per=pd.DataFrame(Per).reset_index()\n",
    "    Per.columns=['Diet','Size']\n",
    "    Per['Percent']=Per.Size/Per.Size.sum()\n",
    "    \n",
    "    return Per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F5. This function excludes the 'n' users previously used in generating a sample training data set from oprior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Remaining_users(S): ## S is an Empirical Sample Generated DataFrame (E1, E2, E3)\n",
    "    Rem=U.loc[~U.user_id.isin(S.user_id)]\n",
    "    return Rem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F6. This function generates simulated sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Simulated_sample(S):  # S isn an empirical sample generated dataframe (E1,E2, E3)\n",
    "    G=S.loc[:, ['user_id', 'order_id', 'prod_per_order']]\n",
    "    G=G.drop_duplicates()\n",
    "    \n",
    "    ## G.prod_per_order is turned into a list n.  n will be used on a loop to generate a simulated order dataframe\n",
    "    n=pd.Series.tolist(G.prod_per_order)\n",
    "\n",
    "    ## This will generate the simulated orders where aisle is randomly picked accoring to its calculated probability or percentage from aisle_percent dataframe\n",
    "    B=[]\n",
    "    for i in n:\n",
    "        Q=np.random.choice(a=aisle_percentb.aisle_id, size=i, p=aisle_percent.aisle_percent_prior)\n",
    "        Q=Q.tolist()\n",
    "        B.append(Q)\n",
    "    \n",
    "    ## The simulated list of randomly picked aisles \"B\" is turned into dataframe \n",
    "        \n",
    "    Sim1=pd.DataFrame(B).reset_index().stack()\n",
    "    Sim1=pd.DataFrame(Sim1).reset_index()\n",
    "    Sim1=Sim1.rename(columns={'level_0':'order_id', 'level_1':'product_num', 0:'aisle_id'})\n",
    "    Sim1=Sim1[Sim1['product_num']!='index']\n",
    "    Sim1['user_id']=S['user_id']\n",
    "    \n",
    "    return Sim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F7. This function will merge two dataframes from two group of proportions being compared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This function will merge two dataframes from two group of proportions being compared \n",
    "def df_prop_compare(M, N):    ## M and N are dataframes with Diet classification with Size and Percent\n",
    "    M.columns=['Diet', 'Size1', 'Percent1']\n",
    "    N.columns=['Diet', 'Size2', 'Percent2']\n",
    "    MN=M.merge(N)\n",
    "    return MN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F8. This function calculates variance, standard deviation, difference in proportion, MOE, degrees of freedom, t_value and p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_std_p_val(W):    ## W is a dataframe with the Diet classification Sizes and Percentages of 2 groups being compared\n",
    "    W['var_1']=W.Percent1*(1-W.Percent1)/W.Size1\n",
    "    W['var_2']=W.Percent2*(1-W.Percent2)/W.Size2\n",
    "    W['var1_2']=W.var_1+W.var_2\n",
    "    W['std_var1_2']=  W.var1_2**0.5             ## a.k.a. standard error\n",
    "\n",
    "    W['%_diff']=abs(W.Percent1-W.Percent2)      ## absolute difference between two proportions\n",
    "\n",
    "    W['moe']=1.96*W.std_var1_2                   ## margin of error\n",
    "\n",
    "    W['DF']= ((W.var_1/W.Size1 +W.var_2/W.Size2)**2)/(((W.var_1/W.Size1)**2/W.Size1)+((W.var_2/W.Size2)**2/W.Size2))  ## degrees of freedom\n",
    "\n",
    "    W['t_val']=(W['%_diff']-0)/W.std_var1_2\n",
    "\n",
    "    W['p_val']=stats.t.sf(np.abs(W.t_val), W.DF)*2  # two-sided pvalue = Prob(abs(t)>tt)\n",
    "\n",
    "    return W   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12B. Identifying Diet Classification by USERS\n",
    "In this section Diets are identified by USERS using their order history.  E1, E2, and E3 are empirical sample dataframes each with 30,000 users.  P_user1, P_user2 and P_user3 are dataframes with the percentage of each diet classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## First Empirical Sample with 30,000 users from oprior DataFrame\n",
    "E1=Sample_maker(U)           # Empirical Sample Generated\n",
    "C1=Diet_Class_user(E1)       # Diets are of 30,000 in the Empirical Sample Classified by USERS\n",
    "P_user1=Diet_Percentage(C1)  # This returns the percentage of the different Diet Classification\n",
    "P_user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Second Empirical Sample with 30,000 users from oprior DataFrame\n",
    "U1=Remaining_users(E1)      # The 30,000 users in the first Emprical Sample (E1) is excluded from the oprior DataFrame\n",
    "\n",
    "E2=Sample_maker(U1)         # Empirical Sample Generated\n",
    "C2=Diet_Class_user(E2)      # Diets are of 30,000 in the Empirical Sample Classified\n",
    "P_user2=Diet_Percentage(C2) # This returns the percentage of the different Diet Classification\n",
    "P_user2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Third Empirical Sample with 30,000 users from oprior DataFrame\n",
    "\n",
    "ET=pd.concat([E1,E2])           # The first two Empirical Sample were merged a\n",
    "U2=Remaining_users(ET)          # The 60,000 users in the first and second Emprical Sample (E1+E2) are excluded from the oprior DataFrame\n",
    "E3=Sample_maker(U2)             # Empirical Sample Generated\n",
    "C3=Diet_Class_user(E3)          # Diets are of 30,000 in the Empirical Sample Classified\n",
    "P_user3=Diet_Percentage(C3)     # This returns the percentage of the different Diet Classification\n",
    "P_user3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph Comparison of % Diet Classification according to USERS across three Empirical Sample Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "X = np.arange(4)\n",
    "plt.bar(X + 0.00, P_user1.Percent, color = 'b', width = 0.25)\n",
    "plt.bar(X + 0.25, P_user2.Percent, color = 'g', width = 0.25)\n",
    "plt.bar(X + 0.50, P_user3.Percent, color = 'r', width = 0.25)\n",
    "plt.ylabel('Percent')\n",
    "\n",
    "ax.set_xticks([p + 1.5 * 0.25 for p in X])\n",
    "ax.set_xticklabels(P_user1.Diet)\n",
    "\n",
    "plt.legend(['P_user1', 'P_user2', 'P_user3'], loc='upper right')\n",
    "plt.title('Diet Distribution of 3 Empirical Sample (user_id)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q12C. Identifying Diet Classification by ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reclassification of Diets using E1 \n",
    "C_order1=Diet_Class_orders(E1)       # Diets are classified per order_id\n",
    "P_order1=Diet_Percentage(C_order1)   # This returns the percentage and a bar graph distribution of the different Diet Classification\n",
    "P_order1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reclassification of Diets using E2 \n",
    "C_order2=Diet_Class_orders(E2)          # Diets are classified per order_id\n",
    "P_order2=Diet_Percentage(C_order2)      # This returns the percentage of the different Diet Classification\n",
    "P_order2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reclassification of Diets using E3 \n",
    "C_order3=Diet_Class_orders(E3)          # Diets are classified per order_id\n",
    "P_order3=Diet_Percentage(C_order3)      # This returns the percentage of the different Diet Classification\n",
    "P_order3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph Comparison of % Diet Classification according to USERS across three Empirical Sample Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Bar Graph comparison of Diet distribution using Empirical Samples classified using ORDERS\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "X = np.arange(4)\n",
    "plt.bar(X + 0.00, P_order1.Percent, color = 'c', width = 0.25)\n",
    "plt.bar(X + 0.25, P_order2.Percent, color = 'm', width = 0.25)\n",
    "plt.bar(X + 0.50, P_order3.Percent, color = 'g', width = 0.25)\n",
    "plt.ylabel('Percent')\n",
    "\n",
    "ax.set_xticks([p + 1.5 * 0.25 for p in X])\n",
    "ax.set_xticklabels(P_order1.Diet)\n",
    "\n",
    "plt.legend(['P_order1', 'P_order2', 'P_order3'], loc='upper right')\n",
    "plt.title('Diet Distribution of 3 Empirical Samples (order_id)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q12D. Generation of Simulated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is the first simulated Sample dataframe with randomly picked aisles\n",
    "Simu1= Simulated_sample(E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simulated_sample orders Diet classified\n",
    "Csim_or=Diet_Class_orders(Simu1)\n",
    "Psimulated_order=Diet_Percentage(Csim_or)    #Percentage of each diet is calculated\n",
    "Psimulated_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simulated_sample users Diet classified\n",
    "Csim_us=Diet_Class_user(Simu1)    \n",
    "Psimulated_user=Diet_Percentage(Csim_us) ## Percentage of each diet calculated\n",
    "Psimulated_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## User_Order_Emp is the dataframe with Sizes and Percentages of Diets classifications from all \"users\" and \"orders\"\n",
    "User_Order_Emp=df_prop_compare(P_user1, P_order1)\n",
    "User_Order_Emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12E. Hypothesis Testing part 1: Are there significant differences between Diet distributions classified by USERS vs. by ORDERS?¶\n",
    "Ho: There is no significant difference in classifying Diets using \"users overall orders\" versus using \"individual orders disregarding who ordered it\"\n",
    "\n",
    "H1:There is significant difference in classifying Diets between using \"users overall orders\" versus using \"individual orders disregarding who ordered it\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will graph Diet Distributions by USERS and by ORDERS using Empirical Samples\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "X = np.arange(4)\n",
    "plt.bar(X + 0.00, User_Order_Emp.Percent1, color = 'b', width = 0.25)\n",
    "plt.bar(X + 0.25, User_Order_Emp.Percent2, color = 'g', width = 0.25)\n",
    "plt.ylabel('Percent')\n",
    "\n",
    "ax.set_xticks([p + 1.5 * 0.25 for p in X])\n",
    "ax.set_xticklabels(User_Order_Emp.Diet)\n",
    "\n",
    "plt.legend(['P_user1', 'P_order1'], loc='upper right')\n",
    "plt.title('Diet Distribution by USERS and by ORDERS of Empirical Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## pvalues, variances, standard deviation, % diff, moe, DF and t_values calculated to test Hypothesis I\n",
    "Use_Or=diff_std_p_val(User_Order_Emp)\n",
    "Use_Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All p_values are <0.05, we can reject the null hypothesis and accept H1.  \n",
    "H1:There is significant difference in classifying Diets between using \"users overall orders\" versus using \"individual orders disregarding who ordered it\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12F. Hypothesis testing part 2: Are there significant differences between Diet distribution classified by USERS and by ORDERS using Empirical dataset vs. Simulated dataset\n",
    "#### I.\n",
    "Ho: There is no significant difference between Diet distribution classified by USERS using Empirical dataset versus Simulated dataset\n",
    "\n",
    "H1: There is significant difference between Diet distribution classified by USERS using Empirical dataset versus Simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sim_Emp_User is the dataframe comparing % of empirical sample and simulated sample both classified by USERS\n",
    "Sim_Emp_User=df_prop_compare(P_user1, Psimulated_user)\n",
    "Sim_Emp_User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will graph Diet Distributions by USERS using Empirical Sample DataSet and Simulated Sample Dataset\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "X = np.arange(4)\n",
    "plt.bar(X + 0.00, Sim_Emp_User.Percent1, color = 'r', width = 0.25)\n",
    "plt.bar(X + 0.25, Sim_Emp_User.Percent2, color = 'm', width = 0.25)\n",
    "plt.ylabel('Percent')\n",
    "\n",
    "ax.set_xticks([p + 1.5 * 0.25 for p in X])\n",
    "ax.set_xticklabels(Sim_Emp_User.Diet)\n",
    "\n",
    "plt.legend(['P_user1', 'Psimulated_user'], loc='upper right')\n",
    "plt.title('Diet Distribution by USERS Empirical and Simulated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## pvalues, variances, standard deviation, % diff, moe, DF and t_values calculated to test Hypothesis II  (by USERS)\n",
    "Sim_Emp=diff_std_p_val(Sim_Emp_User)\n",
    "Sim_Emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All p_values are <0.05, we can reject the null hypothesis and accept H1.  \n",
    "H1: There is significant difference between Diet distribution classified by USERS using Empirical dataset versus Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.\n",
    "Ho: There is no significant difference between Diet distribution classified by ORDERS using Empirical dataset versus Simulated dataset\n",
    "\n",
    "H1: There is significant difference between Diet distribution classified by ORDERS using Empirical dataset versus Simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sim_Emp_Order is the dataframe comparing % of empirical sample and simulated sample both classified by ORDERS\n",
    "Sim_Emp_Order=df_prop_compare(P_order1, Psimulated_order)\n",
    "Sim_Emp_Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will graph Diet Distributions classified by ORDERS using Empirical and Simulated Sample Datasets\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "X = np.arange(4)\n",
    "plt.bar(X + 0.00, Sim_Emp_Order.Percent1, color = 'b', width = 0.25)\n",
    "plt.bar(X + 0.25, Sim_Emp_Order.Percent2, color = 'm', width = 0.25)\n",
    "plt.ylabel('Percent')\n",
    "\n",
    "ax.set_xticks([p + 1.5 * 0.25 for p in X])\n",
    "ax.set_xticklabels(Sim_Emp_Order.Diet)\n",
    "\n",
    "plt.legend(['P_order1', 'Psimulated_order'], loc='upper right')\n",
    "plt.title('Diet Distribution by ORDERS Empirical and Simulated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Sim_Emp_Order is a dataframe with calculated standard deviation, moe, degrees of freedom, t_values and p_values (ORDERS)\n",
    "Sim_Emp_Order=diff_std_p_val(Sim_Emp_Order)\n",
    "Sim_Emp_Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meat_Lovers, NonVegan, and Vegan, p_values are <0.05, we can reject the null hypothesis and accept H1.¶\n",
    "#### H1: There is significant difference between Diet distribution classified by ORDERS using Empirical dataset versus Simulated dataset.  \n",
    "#### However for Pescatarian classified by ORDERS p_value>0.05 which means there is no significant difference in % of this group using Empirical dataset and Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. What is the probability that customer is in a Diet classification given that they purchased from aisle P(Diet|Aisle) (likelihood)? What is the probability that product is from an Aisle given that their Diet is known P(Aisle|Diet)?¶\n",
    "\n",
    "In this section, I used Bayesian Statistics to calculate P(Diet|Aisle) and P(Aisle|Diet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TA is a dataframe with all orders and each user with Diet Classification\n",
    "TA=E1.merge(C1)\n",
    "TA1=TA[TA.Diet=='Meat_Lovers']\n",
    "TA2=TA[TA.Diet=='Pescatarian']\n",
    "TA3=TA[TA.Diet=='NonVegan']\n",
    "TA4=TA[TA.Diet=='Vegan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Probaility of buying from aisles given that they are Meat_Lovers(P(aisle|Meat_Lovers))\n",
    "Prob1=TA1.aisle_id.value_counts()\n",
    "Prob1=pd.DataFrame(Prob1).reset_index()\n",
    "Prob1.columns=['aisle_id', 'count1']\n",
    "Prob1['Prob1']=Prob1['count1']/Prob1['count1'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Probaility of buying from aisles given that they are Pescatarian (P(aisle|Pescatarian))\n",
    "Prob2=TA2.aisle_id.value_counts()\n",
    "Prob2=pd.DataFrame(Prob2).reset_index()\n",
    "Prob2.columns=['aisle_id', 'count2']\n",
    "Prob2['Prob2']=Prob2['count2']/Prob2['count2'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Probaility of buying from aisle 1 given that they are NonVegan (P(aisle|NonVegan))\n",
    "Prob3=TA3.aisle_id.value_counts()\n",
    "Prob3=pd.DataFrame(Prob3).reset_index()\n",
    "Prob3.columns=['aisle_id', 'count3']\n",
    "Prob3['Prob3']=Prob3['count3']/Prob3['count3'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Probaility of buying from aisle 1 given that they are Vegan (P(aisle|Vegan))\n",
    "Prob4=TA4.aisle_id.value_counts()\n",
    "Prob4=pd.DataFrame(Prob4).reset_index()\n",
    "Prob4.columns=['aisle_id', 'count4']\n",
    "Prob4['Prob4']=Prob4['count4']/Prob4['count4'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pmeatlover=P_user1.iloc[0,2]\n",
    "Pnonvegan=P_user1.iloc[1,2]\n",
    "Pvegan=P_user1.iloc[2,2]\n",
    "Pescatarian=P_user1.iloc[3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pall=Prob1.merge(Prob2).merge(Prob3).merge(Prob4).merge(aisle_percentb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pall['P(meat_lover|aisle)']=Pall.Prob1*Pmeatlover/Pall.aisle_percent_prior\n",
    "Pall['P(pescatarian|aisle)']=Pall.Prob2*Pescatarian/Pall.aisle_percent_prior\n",
    "Pall['P(nonvegan|aisle)']=Pall.Prob3*Pnonvegan/Pall.aisle_percent_prior\n",
    "Pall['P(vegan|aisle)']=Pall.Prob4*Pvegan/Pall.aisle_percent_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pall=Pall.set_index('aisle_id').sort_index().reset_index()\n",
    "Pall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This graphs the Aisle number vs P(Diet|Aisle)\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "plt.scatter(range(len(Pall['P(meat_lover|aisle)'])), Pall['P(meat_lover|aisle)'], color='r')\n",
    "plt.scatter(range(len(Pall['P(pescatarian|aisle)'])), Pall['P(pescatarian|aisle)'], color='m')\n",
    "plt.scatter(range(len(Pall['P(nonvegan|aisle)'])), Pall['P(nonvegan|aisle)'], color='b')\n",
    "plt.scatter(range(len(Pall['P(vegan|aisle)'])), Pall['P(vegan|aisle)'], color='y')\n",
    "plt.xlim(0,135)\n",
    "plt.xlabel('aisle_id')\n",
    "plt.ylabel('probaility')\n",
    "plt.title('P(Diet|Aisle)')\n",
    "plt.legend(['Meat_Lover', 'Pescatarian', 'NonVegan', 'Vegan'], loc='upper right')\n",
    "ax.set_xticks(np.arange(0,136,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This graphs aisle number vs P(Aisle|Diet)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.scatter(range(len(Pall['Prob1'])), Pall['Prob1'], color='r')\n",
    "plt.scatter(range(len(Pall['Prob2'])), Pall['Prob2'], color='m')\n",
    "plt.scatter(range(len(Pall['Prob3'])), Pall['Prob3'], color='b')\n",
    "plt.scatter(range(len(Pall['Prob4'])), Pall['Prob4'], color='y')\n",
    "plt.xlim(0,135)\n",
    "plt.xlabel('aisle_id')\n",
    "plt.ylabel('probaility')\n",
    "plt.title('P(Aisle|Diet)')\n",
    "plt.legend(['Meat_Lover', 'Pescatarian', 'NonVegan', 'Vegan'], loc='upper right')\n",
    "ax.set_xticks(np.arange(0,136,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. What products appear in all customer A orders? – These products will have high probability being reordered by customer A\n",
    "allprior dataframe must be used here since Alltrain only have 1 order per user with no order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Percent of products reordered in allprior DataFrame\n",
    "allprior[allprior.reordered==1].shape[0]/allprior.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Percent of products not reordered in Allprior DataFrame\n",
    "allprior[allprior.reordered==0].shape[0]/allprior.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Percent of products reordered in Alltrain DataFrame\n",
    "alltrain[alltrain.reordered==1].shape[0]/alltrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Percent of products not reordered in Alltrain DataFrame\n",
    "alltrain[alltrain.reordered==0].shape[0]/alltrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Reorder_prior=allprior[allprior.reordered==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Products reordered by each users in allprior dataframe. Products with highest reorder count have highest probability of being reordered\n",
    "allprior[allprior.reordered==1].groupby('user_id')['product_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. How many orders for each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allprior.groupby('user_id')['order_number'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. What is the average number of products for each customer per order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltrain.groupby('order_id')['product_name'].size().mean()   ## overall average products per order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltrain.groupby('user_id')['order_id'].size().head()     # number of products ordered per customer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using allprior dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allprior.groupby('order_id')['product_name'].size().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allprior.groupby('user_id')['order_id'].size().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Using average number of product per order for each customer and probability of product to be reordered by customer, can I predict products that will be reordered by customer?\n",
    "What is the accuracy of my predicted next order list to the actual next order list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Users1=otrain.user_id.sample(n=20000, replace=False, random_state=0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "User_train=allprior.loc[allprior.user_id.isin(Users1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## U_order_train is the dataframe of User_train with their number of orders\n",
    "U_order_train=User_train.groupby('user_id')['order_number'].agg(['max'])\n",
    "U_order_train['user_id']=U_order_train.index.get_level_values('user_id').values\n",
    "U_order_train.columns=['max_order', 'user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F9. This function produce a dataframe with predicted orders per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_order(Use_Or):  \n",
    "    ##Train Data with user_id and latest product ordered\n",
    "    ytrain=Use_Or.loc[:,['user_id', 'product_id']]\n",
    "    ytrain.columns=['user_id', 'product_id_latest_train']\n",
    "    \n",
    "    ##Number of products in latest order (Training Data)\n",
    "    Tr=ytrain.user_id.value_counts()\n",
    "    Tr=pd.DataFrame(Tr).reset_index()\n",
    "    Tr.columns=['user_id', 'tQty']\n",
    "\n",
    "    ## DataFrame with the users in  with product_id and Qty of product_id purchased\n",
    "    R=Use_Or.groupby('user_id')['product_id'].value_counts()\n",
    "    R=pd.DataFrame(R)\n",
    "    R.columns=['Qty']\n",
    "    R=R.reset_index()\n",
    "    \n",
    "    ##DataFrame with average product per order\n",
    "    Q=Use_Or.groupby(['user_id', 'order_id'])['product_id'].size()\n",
    "    Q=pd.DataFrame(Q)\n",
    "\n",
    "    Q=Q.reset_index()\n",
    "    p=Q.groupby('user_id')[0].mean()\n",
    "    p=pd.DataFrame(p)\n",
    "    p.columns=['ave_prod_per_order']\n",
    "    p=p.reset_index()\n",
    "    Q=Q.merge(p)\n",
    "    Q['ave_prod_per_order']=Q['ave_prod_per_order'].round(decimals=0)\n",
    "    Q=Q.loc[:,['user_id','order_id', 'ave_prod_per_order']]\n",
    "    Q=Q.loc[:,['user_id', 'ave_prod_per_order']].drop_duplicates()\n",
    "\n",
    "    ## Dataframe with user_id and total product purchased\n",
    "    total_products=Use_Or.user_id.value_counts()\n",
    "    total_products=pd.DataFrame(total_products)\n",
    "    total_products.reset_index(inplace=True)\n",
    "    total_products.columns=['user_id', 'total_products']\n",
    "    total_products.drop_duplicates(inplace=True)\n",
    "    \n",
    "    ## This will compute probability of products to be purchased by user\n",
    "    R=R.merge(total_products)\n",
    "    R['Prob']=R.Qty/R.total_products\n",
    "    \n",
    "\n",
    "    ## This will predict next order list using calculated probabilities of products for each user and using average product per order\n",
    "    B=[]\n",
    "    user=pd.Series.tolist(Q.user_id)\n",
    "    n=pd.Series.tolist(Q.ave_prod_per_order) ## This is a list of average product per order per user\n",
    "\n",
    "    i=0\n",
    "    for i in range(len(n)):\n",
    "        H=R[R.user_id==user[i]]\n",
    "        K=np.random.choice(a=H.product_id, size=n[i], p=H.Prob, replace=False)\n",
    "        K=K.tolist()\n",
    "        B.append(K)\n",
    "        i+=1\n",
    "    ## This is the predicted latest order\n",
    "    Reord=pd.DataFrame(B, index=Q.user_id).stack()\n",
    "    Reord=pd.DataFrame(Reord, columns=['product_id'])\n",
    "    Reord=Reord.reset_index()\n",
    "    Reord.drop('level_1', axis=1, inplace=True)\n",
    "    \n",
    "    return Reord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## produce a dataframe that has probability of product being purchased by specific user\n",
    "U_pred=predict_order(User_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F10. This function reformats dataframe with predicted products two columns where predicted products are all written accross the user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predicted_product_format(predicted):\n",
    "    pred = ''\n",
    "    for product in predicted:\n",
    "        if product > 0:\n",
    "            pred = pred + str(int(product)) + ' '\n",
    "    \n",
    "    if pred != '':\n",
    "        return pred.rstrip()\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this creates a DataFrame of user_id and predicted product_id list \n",
    "predicted_order = pd.DataFrame(U_pred.groupby('user_id')[\"product_id\"].apply(predicted_product_format)).reset_index()\n",
    "predicted_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This generates the actual product ordered of Users1 in alltrain dataset\n",
    "train_order = pd.DataFrame(User_train.groupby('user_id')[\"product_id\"].apply(predicted_product_format)).reset_index()\n",
    "train_order.columns=['user_id', 'product_id_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine_pred_train=predicted_order.merge(train_order)\n",
    "combine_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F11. This function calculates cosine similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "## Cosine similarity function to compare % similarities in the train and predicted products\n",
    "\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " i=0\n",
    "cs=[]\n",
    "st=combine_pred_train.product_id_train.values.tolist()\n",
    "sp=combine_pred_train.product_id.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F12. This function calculates cosine similarity score for all rows in the dataframe with columns of predicting and training order lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim_score(D):\n",
    "    for i in range(len(st)):\n",
    "        st[i]=st[i].split()\n",
    "        sp[i]=sp[i].split()\n",
    "        a=Counter(st[i])\n",
    "        b=Counter(sp[i])\n",
    "        cs.append(counter_cosine_similarity(a,b))\n",
    "        i+=1\n",
    "    return pd.Series(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## cosine_similarity_score is added to the combine_pred_train dataframe\n",
    "combine_pred_train['cosine_similarity_score']=cos_sim_score(combine_pred_train)\n",
    "combine_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine_pred_train.cosine_similarity_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F13. These two functions calculates F1 score for each row of a dataframe with predicted and training set of next order list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    y_true = set(y_true)\n",
    "    y_pred = set(y_pred)\n",
    "    cross_size = len(y_true & y_pred)\n",
    "    if cross_size == 0: return 0.\n",
    "    p = 1. * cross_size / len(y_pred)\n",
    "    r = 1. * cross_size / len(y_true)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def f1_score(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This for loop is to calculate the F1_score for all 30,000 users in the combine_pred_train dataframe\n",
    "i=0\n",
    "F=[]\n",
    "for i in range(len(st)):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    a.append(st[i])\n",
    "    b.append(sp[i])\n",
    "    f1=f1_score(a, b)\n",
    "    F.append(f1)\n",
    "    i+=1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine_pred_train['F1_score']=pd.Series(F)\n",
    "combine_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is the mean F1_score of all predicted orders of users \n",
    "np.mean(F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
